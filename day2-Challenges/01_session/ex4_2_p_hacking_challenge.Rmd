---
title: "Exercise 4.2: P-hacking challenge"
output: David Souto
output: html_document
---

Learning Objectives:
Understand what p-hacking is and why it is problematic.
Explore the p-hacker Shiny app to see how p-hacking can lead to significant results.
Simulate p-hacking strategies in R to understand their effects on false positives.
Reflect on the ethical implications of p-hacking in scientific research.
Part 1: What is P-Hacking?
P-hacking refers to the practice of manipulating data analysis to achieve statistically significant results (p < 0.05). This can involve:
Trying multiple statistical tests.
Adding or removing variables.
Excluding outliers.
Collecting more data until significance is achieved.
While these practices may seem harmless, they inflate the false positive rate and undermine the reliability of scientific findings.
Part 2: Interactive Exploration with the P-Hacker App
Step 1: Visit the P-Hacker App
The p-hacker Shiny app allows you to practice p-hacking in a controlled environment. You can access the app here: P-Hacker App [[1]].
Step 2: Instructions
Start a new study in the app.
Use the tools provided (e.g., outlier exclusion, adding covariates, collecting more data) to achieve a significant result (p < 0.05).
Record how many steps it took you to find a significant result.
Questions:
How many steps did it take you to achieve a significant result?
Did you find it easy or difficult to manipulate the data to achieve significance?
Reflect: Does this exercise make you question the reliability of published research findings?
Part 3: Simulating P-Hacking in R
Exercise 1: Simulating Multiple Hypothesis Tests
In this exercise, we simulate a scenario where a researcher tests multiple hypotheses on the same dataset. Complete the code below to see how often a significant result is found by chance.
# Function to simulate multiple hypothesis tests simulate_p_hacking <- function(n_tests = 100, sample_size = 30, true_effect = 0) { p_values <- numeric(n_tests) for (i in 1:n_tests) { # Generate two groups of data group1 <- rnorm(sample_size, mean = 0, sd = 1) group2 <- rnorm(sample_size, mean = true_effect, sd = 1) # Perform t-test test_result <- t.test(group1, group2) p_values[i] <- test_result$p.value } return(p_values) } # Simulate p-hacking p_values <- simulate_p_hacking(n_tests = 100) # Visualize the distribution of p-values ggplot(data.frame(p_value = p_values), aes(x = p_value)) + geom_histogram(bins = 30, fill = "skyblue", color = "black") + geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") + labs(title = "Distribution of P-Values from Multiple Hypothesis Tests", x = "P-Value", y = "Frequency")
Questions:
What proportion of p-values are below 0.05?
If there is no true effect (true_effect = 0), why do we still see significant results?
Exercise 2: Adding Covariates to Achieve Significance
Another common p-hacking strategy is to add covariates to a model until significance is achieved. Complete the code below to simulate this scenario.
# Function to simulate p-hacking by adding covariates simulate_covariate_p_hacking <- function(n_covariates = 10, sample_size = 30, true_effect = 0) { p_values <- numeric(n_covariates) # Generate data group <- rep(c(0, 1), each = sample_size / 2) outcome <- rnorm(sample_size, mean = true_effect * group, sd = 1) for (i in 1:n_covariates) { # Generate a random covariate covariate <- rnorm(sample_size) # Fit a linear model with the covariate model <- lm(outcome ~ group + covariate) p_values[i] <- summary(model)$coefficients["group", "Pr(>|t|)"] } return(p_values) } # Simulate p-hacking with covariates p_values <- simulate_covariate_p_hacking() # Visualize the distribution of p-values ggplot(data.frame(p_value = p_values), aes(x = p_value)) + geom_histogram(bins = 30, fill = "lightgreen", color = "black") + geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") + labs(title = "P-Values After Adding Covariates", x = "P-Value", y = "Frequency")
Questions:
How does adding covariates affect the likelihood of finding a significant result?
Why is this practice problematic?
Part 4: Reflection and Discussion
Ethical Implications
How does p-hacking undermine the credibility of scientific research?
What steps can researchers take to avoid p-hacking?
How can practices like pre-registration and replication help combat p-hacking?
Key Takeaways
P-hacking can easily lead to significant results, even when there is no true effect.
Practices like testing multiple hypotheses or adding covariates inflate the false positive rate.
Tools like the p-hacker app demonstrate how easy it is to manipulate data to achieve significance.
Ethical research practices, such as pre-registration and transparency, are essential to maintaining the integrity of science.

We are going to look explore some of the characteristics of the packages we installed, in particular those are are part of the *tidyverse*

If you didn't install this already:

* Set up the working directory to the day-1 folder: click on the "Session" tab (on top) then "Set working Directory" then "Choose Directory" then "Open".

* Run source("workshop_pack.R") if you didn't install the packages yet.

```{r}

# we are telling R to source (run) the file that is two folders up with "../.."
source("../../Pre_Package_Install.R")

```

### Start by loading the tidyverse

The tidyverse is a collection of R packages designed for data science that share a common philosophy and design. In this worksheet, we'll explore:
- What packages are included in the tidyverse
- How to handle package conflicts
- Compare tidyverse with base R approaches

```{r}
# Use Ctrl+Enter (Cmd+Enter) to run line-by-line, rather than by chunk
library("tidyverse"); 

# find out how to cite in a publication (this format can be imported to Zotero for instance)
citation("tidyverse")

# Optional: Find out general information about the package with library(help="tidyverse"), find out what a meme is with library(help="meme")

```

Look at the message that appears when loading tidyverse. Which core packages are loaded by default? Take note of conflicts. Those are functions within one of the packages that have the same name as other loaded packages (e.g. part of base R)

### Managing package conflicts
There are different ways to manage naming conflicts. Or you could be explicit about those functions by using the "package_name"::"function name" syntax. You could also install the 'conflicted' package, which will throw an error when 'overloaded' functions are found. 

```{r}
# Run this chunk and observe the output 
filter(mpg, cyl==4) # we are filtering the pre-loaded fuel economy data frame by number of cylinders

# Do the same using "filter" within the dplyr package by using the :: syntax, 

# Do the same using the "stats" package  

# Optional: install the conflicted package (syntax: install.packages(""), then load with library()) and observe the output when running the first line

# Optional: Can you guess what dplyr is about by browsing through package functions with the :: syntax? 
```

### Data filtering

Let's see how we filter data in the tydiverse by selecting cars with 6 cylinders:

```{r}
# Base R approach 
base_r_way <-mtcars[mtcars$cyl == 6, ]

# Tidyverse approach 
tidyverse_way <- mtcars %>% dplyr::filter(cyl == 6) 

# Compare the results 
head(base_r_way) # head() shows the first rows of a dataset
head(tidyverse_way)

```

### Data Summaries

Calculate the average mpg by cylinder:

```{r}
# Base R approach 
aggregate(mpg ~ cyl, data = mtcars, FUN = mean) 

```

```{r}
# Tidyverse approach 
mtcars %>% group_by(cyl) %>% summarise(avg_mpg = mean(mpg))

```

Reflection Questions (2 minutes)

What differences do you notice between tidyverse and base R syntax?
Which approach seems more readable to you and why?
Can you identify situations where one approach might be preferable over the other?

Key Takeaways
The tidyverse is a collection of packages that work together cohesively.
Keep in mind possible function naming conflicts 
The tidyverse approach often emphasizes readability 

Additional Resources
R for Data Science (free online): https://r4ds.had.co.nz/
Tidyverse website: https://www.tidyverse.org/
RStudio's cheatsheets: https://www.rstudio.com/resources/cheatsheets/

---
title: "Exercise 4.2: P-hacking challenge"
output: David Souto
output: html_document
---

In this worksheet we are going to explore issues with researcher degrees of freedom to:
- Understand what p-hacking is and why it is problematic.
- Explore the p-hacker Shiny app to see how p-hacking can lead to significant results.
- Simulate p-hacking strategies in R to understand their effects on false positives.

# What is P-Hacking?

P-hacking refers to the practice of manipulating data analysis to achieve statistically significant results (p < 0.05). This can involve:
- Trying multiple statistical tests.
- Adding or removing variables.
- Excluding outliers.
- Collecting more data until significance is achieved.

While these practices may seem harmless, they inflate the false positive rate and undermine the reliability of scientific findings.

# Interactive Exploration with the P-Hacker App

The p-hacker Shiny app allows you to practice p-hacking in a controlled environment. You can access the app here: P-Hacker App [[1]].

Instructions
- Start a new study in the app.
- Use the tools provided (e.g., outlier exclusion, adding covariates, collecting more data) to achieve a significant result (p < 0.05).
- Record how many steps it took you to find a significant result.

Questions:

How many steps did it take you to achieve a significant result?

Did you find it easy or difficult to manipulate the data to achieve significance?

Reflect: Does this exercise make you question the reliability of published research findings?

# Simulating P-Hacking in R

Exercise 1: Simulating Multiple Hypothesis Tests

In this exercise, we simulate a scenario where a researcher tests multiple hypotheses on the same dataset. Complete the code below to see how often a significant result is found by chance.

```{r}
simulate_p_hacking <- function(n_experiments = 100, n_tests = 10, sample_size = 30, true_effect = 0) {
  # Matrix to store p-values for each experiment and test
  p_values_matrix <- matrix(nrow = n_experiments, ncol = n_tests)
  
  for (i in 1:n_experiments) {
    for (j in 1:n_tests) {
      # Generate two groups of data
      group1 <- rnorm(sample_size, mean = 0, sd = 1)
      group2 <- rnorm(sample_size, mean = true_effect, sd = 1)
      # Perform t-test
      test_result <- t.test(group1, group2)
      p_values_matrix[i,j] <- test_result$p.value
    }
  }
  return(p_values_matrix)
}

# Run simulation
n_experiments <- 100
n_tests <- 5
p_values_matrix <- simulate_p_hacking(n_experiments = n_experiments, n_tests = n_tests)

# Calculate number of experiments with at least one significant result
experiments_with_sig <- sum(apply(p_values_matrix, 1, function(x) any(x < 0.05)))

# Create data for plotting
p_values_long <- data.frame(p_value = as.vector(p_values_matrix))

# Visualize the distribution of p-values
ggplot(p_values_long, aes(x = p_value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") +
  annotate("text", x = 0.1, y = max(hist(p_values_long$p_value, breaks = 30, plot = FALSE)$counts),
           label = paste("Experiments with at least\none significant result:", experiments_with_sig,
                        "\nout of", n_experiments, "experiments\n(",
                        round(100 * experiments_with_sig/n_experiments, 1), "%)"),
           hjust = 0) +
  labs(title = "Distribution of P-Values from Multiple Experiments",
       subtitle = paste(n_tests, "tests per experiment"),
       x = "P-Value",
       y = "Frequency")
```

# Function to simulate multiple hypothesis tests 

Questions:
What proportion of p-values are below 0.05?
If there is no true effect (true_effect = 0), why do we still see significant results?
Exercise 2: Adding Covariates to Achieve Significance
Another common p-hacking strategy is to add covariates to a model until significance is achieved. Complete the code below to simulate this scenario.
# Function to simulate p-hacking by adding covariates simulate_covariate_p_hacking <- function(n_covariates = 10, sample_size = 30, true_effect = 0) { p_values <- numeric(n_covariates) # Generate data group <- rep(c(0, 1), each = sample_size / 2) outcome <- rnorm(sample_size, mean = true_effect * group, sd = 1) for (i in 1:n_covariates) { # Generate a random covariate covariate <- rnorm(sample_size) # Fit a linear model with the covariate model <- lm(outcome ~ group + covariate) p_values[i] <- summary(model)$coefficients["group", "Pr(>|t|)"] } return(p_values) } # Simulate p-hacking with covariates p_values <- simulate_covariate_p_hacking() # Visualize the distribution of p-values ggplot(data.frame(p_value = p_values), aes(x = p_value)) + geom_histogram(bins = 30, fill = "lightgreen", color = "black") + geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") + labs(title = "P-Values After Adding Covariates", x = "P-Value", y = "Frequency")
Questions:
How does adding covariates affect the likelihood of finding a significant result?
Why is this practice problematic?
Part 4: Reflection and Discussion
Ethical Implications
How does p-hacking undermine the credibility of scientific research?
What steps can researchers take to avoid p-hacking?
How can practices like pre-registration and replication help combat p-hacking?
Key Takeaways
P-hacking can easily lead to significant results, even when there is no true effect.
Practices like testing multiple hypotheses or adding covariates inflate the false positive rate.
Tools like the p-hacker app demonstrate how easy it is to manipulate data to achieve significance.
Ethical research practices, such as pre-registration and transparency, are essential to maintaining the integrity of science.

We are going to look explore some of the characteristics of the packages we installed, in particular those are are part of the *tidyverse*

---
title: "Exercise 4.2: P-hacking challenge"
output: David Souto
output: html_document
---

In this worksheet we are going to explore issues with researcher degrees of freedom to:
- Understand what p-hacking is and why it is problematic.
- Explore the p-hacker Shiny app to see how p-hacking can lead to significant results.
- Simulate p-hacking strategies in R to understand their effects on false positives.

# What is P-Hacking?

P-hacking refers to the practice of manipulating data analysis to achieve statistically significant results (p < 0.05). This can involve:
- Trying multiple statistical tests.
- Adding or removing variables.
- Excluding outliers.
- Collecting more data until significance is achieved.

While these practices may seem harmless, they inflate the false positive rate and undermine the reliability of scientific findings.

# Interactive Exploration with the P-Hacker App

The p-hacker Shiny app allows you to practice p-hacking in a controlled environment. You can access the app here: [P-Hacker App.](https://shinyapps.org/apps/p-hacker/)

Instructions
- Start a new study in the app.
- Use the tools provided (e.g., outlier exclusion, adding covariates, collecting more data) to achieve a significant result (p < 0.05).
- Record how many steps it took you to find a significant result.

Questions:

How many steps did it take you to achieve a significant result?

Did you find it easy or difficult to manipulate the data to achieve significance?

Does this exercise make you question the reliability of published research findings?

# Simulating P-Hacking in R

In this exercise, we simulate a scenario where a researcher tests multiple hypotheses on the same dataset with an effect of 0. 

See how the simulation is implemented and run the code to see how often a significant result is found by chance.

```{r}
simulate_p_hacking <- function(n_experiments = 100, n_tests = 10, sample_size = 30, true_effect = 0) {
  # Matrix to store p-values for each experiment and test
  p_values_matrix <- matrix(nrow = n_experiments, ncol = n_tests)
  
  for (i in 1:n_experiments) {
    for (j in 1:n_tests) {
      # Generate two groups of data
      group1 <- rnorm(sample_size, mean = 0, sd = 1)
      group2 <- rnorm(sample_size, mean = true_effect, sd = 1)
      # Perform t-test
      test_result <- t.test(group1, group2)
      p_values_matrix[i,j] <- test_result$p.value
    }
  }
  return(p_values_matrix)
}

# Run simulation
n_experiments <- 100
n_tests <- 5
p_values_matrix <- simulate_p_hacking(n_experiments = n_experiments, n_tests = n_tests)

# Calculate number of experiments with at least one significant result
experiments_with_sig <- sum(apply(p_values_matrix, 1, function(x) any(x < 0.05)))

# Create data for plotting
p_values_long <- data.frame(p_value = as.vector(p_values_matrix))

# Visualize the distribution of p-values
ggplot(p_values_long, aes(x = p_value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_vline(xintercept = 0.05, color = "red", linetype = "dashed") +
  annotate("text", x = 0.1, y = max(hist(p_values_long$p_value, breaks = 30, plot = FALSE)$counts),
           label = paste("Experiments with at least\none significant result:", experiments_with_sig,
                        "\nout of", n_experiments, "experiments\n(",
                        round(100 * experiments_with_sig/n_experiments, 1), "%)"),
           hjust = 0) +
  labs(title = "Distribution of P-Values from Multiple Experiments",
       subtitle = paste(n_tests, "tests per experiment"),
       x = "P-Value",
       y = "Frequency")
```

Questions:

What proportion of p-values are below 0.05? How does change with the number of tests? 

Key takeways 

P-hacking can easily lead to significant results, even when there is no true effect.

Practices like testing multiple hypotheses or adding covariates inflate the false positive rate.

Tools like the p-hacker app demonstrate how easy it is to manipulate data to achieve significance.